\section{Описание методов}
\label{sec:methods}

Градиентные методы основаны на идее аппроксимации минимизируемой функции её разложением в текущей точке, удерживая члены до определённого порядка.
В случае методов второго порядка, аппроксимация функции $f$ в точке $x_k$ выглядит так:
\begin{equation}
    f(x) \approx f(x_k) + \nabla^{T}f(x_k)(x - x_k) + (x - x_k)^{T}H(x_k)(x - x_k)/2
    \label{eq:taylor_decomposition}
\end{equation}
\noindent где $\nabla{f(x_k)}$ - значение градиента, а $H(x_k)$ - матрица Гессе, в точке разложения.\\
Предполагая, что данная функция принадлежит по крайней мере $C^2$ и выпукла, делаем вывод, что и её приближение выпукло.
Значит оно имеет единственный минимум $x_*$.[3]\\
Из \eqref{eq:taylor_decomposition} элементарно вычисляется оптимизирующий вектор $p_k = x_* - x_k$:\\
\begin{equation}
    p_k = -(H(x_k))^{-1}{\nabla}f(x_k)
    \label{eq:aprox_optim_vector}
\end{equation}

\subsection{Градиентный метод Ньютона}
\label{subsec:theory_newton}

Метод ньютона предполагает следующее правило построения минимизирующей последовательности:
\begin{enumerate}
    \item $x_k = x_0$
    \item Если условие окончания не выполнено - дальше
    \item $p_k = -(H(x_k))^{-1}{\nabla}f(x_k)$
    \item $x_{k+1} = x_k + p_k$
    \item  $k = k + 1$
    \item Возврат к пункту 2
\end{enumerate}
$x_0$ - начальное приближение

\subsection{Градиентный метод Ньютона с дроблением шага (Пшеничного)}
\label{subsec:theory_pschen}
Данная модификация метода Ньютона предлагает перед переходом к следующему шагу,
дробить длину шага пока не будет выполнено особое условие.

Алгоритм построения последовательности:
\begin{enumerate}
    \item $x_k = x_0$
    \item Если условие окончания не выполнено - дальше
    \item $p_k = -(H(x_k))^{-1}{\nabla}f(x_k)$
    \item $\alpha = 1$
    \item Если $f(x_k + \alpha p_k) - f(x_k) \leq \alpha \delta \nabla^{T}f(x_k)p_x$ - переход к пункту 6
    \item $\alpha = \alpha \lambda$
    \item Возврат к пункту 5
    \item $x_{k+1} = x_k + \alpha p_k$
    \item Возврат к пункту 2
\end{enumerate}
$x_0$ - начальное приближение, $0 <\delta\leq \frac{1}{2}, 0 <\lambda <1$ - параметры дробления.

\subsection{Градиентный метод БФГШ}
\label{subsec:theory_bfgs}
В приложениях исходная функция, как правило, не задаётся формулой, а определяется в ходе измерений.
Поэтому матрицу Гессе тоже не получить символьно, а её вычисление требует довольно много обращений к функции.
К тому же на каждом шаге необходимо её обращать.

Градиентный метод Бройдена - Флетчера - Гольдфарба - Шанно(БФГШ) относится к квазиньютоновским и помогает обойти эти трудности.

Вместо точного вчисления матрицы Гессе и её обращения, строится последовательность матриц, которая  [1] сходится к обращённой матрице гессе.

Схема вычисления[1]:
\begin{equation}
C_{k + 1} = (I - \rho_k s_k y_k^T)C_k(I - \rho_k y_k s_k^T) + \rho_k s_k s_k^T,
\label{eq:bfgs_mat}
\end{equation}
где $\rho_k = \frac{1}{y_k^T s_k}$, $I$ - единичная матрица, $s_k = x_{k + 1} - x_k$ - шаг алгоритма на итерации, $y_k = \nabla f_{k + 1} - \nabla f_{k}$ - изменение градиента на итерации.
В качестве начального приближения матрицы можно взять единичную.

Также такая матрица обладает свойством самокоррекции, если длина шага выбирается одномерным поиском минимума функции по найденному направлению $p_k$:

\begin{equation}
f(x_k + \alpha_k p_k) = min_{\alpha \geq 0} f(x_k + \alpha p_k)
\end{equation}

В итоге алгоритм выглядит следующим образом:

\begin{enumerate}
    \item $x_k = x_0$
    \item $C_k = C_0$
    \item Если условие окончания не выполнено - дальше
    \item $p_k = -(C_k){\nabla}f(x_k)$
    \item $\alpha = 1$
    \item Поиск оптимального $\alpha_x f(x_k + \alpha_k p_k) = min_{\alpha \geq 0} f(x_k + \alpha p_k)$
    \item $s_k = \alpha p_k$
    \item $x_{k+1} = x_k + s_k$
    \item Обновить $C_{k+1}$ по формуле \eqref{eq:bfgs_mat}
    \item Возврат к пункту 2
\end{enumerate}

Для всех методов существуют вариации, в которых матрица вычисляется не на каждом шаге, но в нашей работе мы вычисляем её каждый раз.